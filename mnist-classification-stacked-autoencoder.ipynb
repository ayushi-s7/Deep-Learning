{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- Basic code for MNIST classification where for feature engineering stacked vanilla autoencoder is used.\n- There are many optimization possible pertaining to architecture, training and hyperparameter tuning. \n- We can also run it for more epochs.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nfrom torchvision.transforms import ToTensor\nfrom matplotlib import pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-28T04:13:46.027329Z","iopub.execute_input":"2024-03-28T04:13:46.027700Z","iopub.status.idle":"2024-03-28T04:13:46.033300Z","shell.execute_reply.started":"2024-03-28T04:13:46.027673Z","shell.execute_reply":"2024-03-28T04:13:46.032179Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device=torch.device(type='cuda',index=0)\nelse:\n    device=torch.device(type='cpu',index=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T04:13:46.034957Z","iopub.execute_input":"2024-03-28T04:13:46.035293Z","iopub.status.idle":"2024-03-28T04:13:46.044325Z","shell.execute_reply.started":"2024-03-28T04:13:46.035270Z","shell.execute_reply":"2024-03-28T04:13:46.043521Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_dataset=datasets.MNIST(root=\"/kaggle/temp/\",\n                            train=True,\n                            download=True,\n                            transform=ToTensor())\n\neval_dataset=datasets.MNIST(root=\"/kaggle/temp/\",\n                            train=False,\n                            download=True,\n                            transform=ToTensor())\n\nbatch_size=64\n\ntrain_dataloader=DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\neval_dataloader=DataLoader(dataset=eval_dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T04:13:46.045285Z","iopub.execute_input":"2024-03-28T04:13:46.045562Z","iopub.status.idle":"2024-03-28T04:13:46.137409Z","shell.execute_reply.started":"2024-03-28T04:13:46.045539Z","shell.execute_reply":"2024-03-28T04:13:46.136409Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Encoder1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lkrelu=nn.LeakyReLU(0.1)\n        self.l1=nn.Linear(in_features=784,out_features=512)\n        self.l2=nn.Linear(in_features=512,out_features=256)\n    \n    def forward(self,x):\n        net1=self.l1(x)\n        out1=self.lkrelu(net1)\n        net2=self.l2(out1)\n        out2=self.lkrelu(net2)\n        return out2\n    \nclass Decoder1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lkrelu=nn.LeakyReLU(0.1)\n        self.sig=nn.Sigmoid()\n        self.l1=nn.Linear(in_features=256,out_features=512)\n        self.l2=nn.Linear(in_features=512,out_features=784)\n    \n    def forward(self,x):\n        net1=self.l1(x)\n        out1=self.lkrelu(net1)\n        net2=self.l2(out1)\n        out2=self.sig(net2)\n        return out2\n\nclass Encoder2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lkrelu=nn.LeakyReLU(0.1)\n        self.l1=nn.Linear(in_features=256,out_features=100)\n            \n    def forward(self,x):\n        net1=self.l1(x)\n        out1=self.lkrelu(net1)\n        return out1\n    \nclass Decoder2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lkrelu=nn.LeakyReLU(0.1)\n        self.l1=nn.Linear(in_features=100,out_features=256)\n            \n    def forward(self,x):\n        net1=self.l1(x)\n        out1=self.lkrelu(net1)\n        return out1\n\n\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.out=nn.Linear(in_features=100,out_features=10)\n    \n    def forward(self,x):\n        out=self.out(x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-03-28T04:13:46.138961Z","iopub.execute_input":"2024-03-28T04:13:46.139281Z","iopub.status.idle":"2024-03-28T04:13:46.152549Z","shell.execute_reply.started":"2024-03-28T04:13:46.139257Z","shell.execute_reply":"2024-03-28T04:13:46.151557Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"lossmse_fn=nn.MSELoss()\nlossentropy_fn=nn.CrossEntropyLoss()\nlr=0.001\n\ne1=Encoder1().to(device)\nd1=Decoder1().to(device)\ne2=Encoder2().to(device)\nd2=Decoder2().to(device)\nclf=Classifier().to(device)\n\nopte1=Adam(params=e1.parameters(),lr=lr)\noptd1=Adam(params=d1.parameters(),lr=lr)\nopte2=Adam(params=e2.parameters(),lr=lr)\noptd2=Adam(params=d2.parameters(),lr=lr)\noptclf=Adam(params=clf.parameters(),lr=lr)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T04:13:46.154573Z","iopub.execute_input":"2024-03-28T04:13:46.154844Z","iopub.status.idle":"2024-03-28T04:13:46.334824Z","shell.execute_reply.started":"2024-03-28T04:13:46.154822Z","shell.execute_reply":"2024-03-28T04:13:46.334059Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#train AutoEncoder1\ndef train_e1d1():    \n    track_loss=0\n    \n    e1.train()\n    d1.train()\n    \n    for i,(x,_) in enumerate(train_dataloader):\n        x=torch.reshape(x,shape=(-1,784))\n        x=x.to(device)\n\n        latent=e1(x)\n        pred=d1(latent)\n            \n        loss=lossmse_fn(pred,x)\n        \n        track_loss+=loss.item()\n          \n        loss.backward()\n        \n        opte1.step()\n        optd1.step()\n        \n        opte1.zero_grad()\n        optd1.zero_grad()\n        \n        running_loss=track_loss/(i+(x.shape[0]/batch_size))\n            \n    return round(running_loss,4)\n\n#train AutoEncoder2\ndef train_e2d2():    \n    track_loss=0\n    \n    e2.train()\n    d2.train()\n    \n    for i,(x,_) in enumerate(train_dataloader):\n        x=torch.reshape(x,shape=(-1,784))\n        x=x.to(device)\n\n        latente1=e1(x)\n        latente2=e2(latente1.detach())\n        pred=d2(latente2)\n            \n        loss=lossmse_fn(pred,latente1.detach()) \n        \n        track_loss+=loss.item()\n        \n        loss.backward()\n        \n        opte2.step()\n        optd2.step()\n        opte2.zero_grad()\n        optd2.zero_grad()\n        \n        running_loss=track_loss/(i+(x.shape[0]/batch_size))\n    \n    return round(running_loss,4)\n\n#train Classifier\ndef train_clf():  \n    track_loss=0\n    num_correct=0\n    \n    clf.train()\n    \n    for i,(x,y) in enumerate(train_dataloader):\n        x=torch.reshape(x,shape=(-1,784))\n        x=x.to(device)\n        y=y.to(device)\n\n        latente1=e1(x)\n        latente2=e2(latente1)\n        pred=clf(latente2.detach())\n        \n        num_correct+=(torch.argmax(pred,dim=1)==y).type(torch.float).sum().item()\n            \n        loss=lossentropy_fn(pred,y)\n        \n        track_loss+=loss.item()\n        \n        loss.backward()\n        \n        optclf.step()\n        optclf.zero_grad()\n        \n        running_loss=track_loss/(i+(x.shape[0]/batch_size))\n        running_acc=(num_correct/((i*batch_size+x.shape[0])))*100\n    \n    return round(running_loss,4), round(running_acc,2)\n        \n#finetune whole \ndef train_whole():\n    track_loss=0\n    num_correct=0\n    \n    e1.train()\n    e2.train()\n    clf.train()\n    \n    for i,(x,y) in enumerate(train_dataloader):\n        x=torch.reshape(x,shape=(-1,784))\n        x=x.to(device)\n        y=y.to(device)\n\n        latente1=e1(x)\n        latente2=e2(latente1)\n        pred=clf(latente2)\n        \n        num_correct+=(torch.argmax(pred,dim=1)==y).type(torch.float).sum().item()\n            \n        loss=lossentropy_fn(pred,y)\n        \n        track_loss+=loss.item()\n        \n        loss.backward()\n        \n        opte1.step()\n        opte2.step()\n        optclf.step()\n        \n        opte1.zero_grad()\n        opte2.zero_grad()\n        optclf.zero_grad()\n        \n        running_loss=track_loss/(i+(x.shape[0]/batch_size))\n        running_acc=(num_correct/((i*batch_size+x.shape[0])))*100\n    \n    return round(running_loss,4),round(running_acc,2)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T04:13:46.336084Z","iopub.execute_input":"2024-03-28T04:13:46.336682Z","iopub.status.idle":"2024-03-28T04:13:46.357474Z","shell.execute_reply.started":"2024-03-28T04:13:46.336648Z","shell.execute_reply":"2024-03-28T04:13:46.356539Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Eval AutoEncoder1\ndef eval_e1d1():\n    track_loss=0\n    \n    e1.eval()\n    d1.eval()\n    \n    with torch.no_grad():\n        for i,(x,_) in enumerate(eval_dataloader):\n            x=torch.reshape(x,shape=(-1,784))\n            x=x.to(device)\n\n            latent=e1(x)\n            pred=d1(latent)\n\n            loss=lossmse_fn(pred,x)\n            track_loss+=loss.item()\n            \n            running_loss=track_loss/(i+(x.shape[0]/batch_size))\n    \n    return round(running_loss,4),x,pred\n\n#Eval AutoEncoder2\ndef eval_e2d2():\n    track_loss=0\n    \n    e2.eval()\n    d2.eval()\n    \n    with torch.no_grad():\n        for i,(x,_) in enumerate(eval_dataloader):\n            x=torch.reshape(x,shape=(-1,784))\n            x=x.to(device)\n\n            latente1=e1(x)\n            latente2=e2(latente1)\n            pred=d2(latente2)\n\n            loss=lossmse_fn(pred,latente1)\n            \n            track_loss+=loss.item()\n            \n            running_loss=track_loss/(i+(x.shape[0]/batch_size))\n    \n    return round(running_loss,4)\n            \n        \n#Eval Classifier\ndef eval_clf():\n    track_loss=0\n    num_correct=0\n    \n    clf.eval()\n    \n    with torch.no_grad():\n        for i,(x,y) in enumerate(eval_dataloader):\n            x=torch.reshape(x,shape=(-1,784))\n            x=x.to(device)\n            y=y.to(device)\n\n            latente1=e1(x)\n            latente2=e2(latente1)\n            pred=clf(latente2)\n            \n            num_correct+=(torch.argmax(pred,dim=1)==y).type(torch.float).sum().item()\n            running_acc=(num_correct/((i*batch_size+x.shape[0])))*100\n            \n            loss=lossentropy_fn(pred,y)\n            track_loss+=loss.item()\n            \n            running_loss=track_loss/(i+(x.shape[0]/batch_size))\n            \n    \n    return round(running_loss,4), round(running_acc,2)\n        \n#eval whole \ndef eval_whole():\n    track_loss=0\n    num_correct=0\n    \n    e1.eval()\n    e2.eval()\n    clf.eval()\n    with torch.no_grad():\n        for i,(x,y) in enumerate(eval_dataloader):\n            x=torch.reshape(x,shape=(-1,784))\n            x=x.to(device)\n            y=y.to(device)\n\n            latente1=e1(x)\n            latente2=e2(latente1)\n            pred=clf(latente2)\n            \n            num_correct+=(torch.argmax(pred,dim=1)==y).type(torch.float).sum().item()\n            running_acc=(num_correct/((i*batch_size+x.shape[0])))*100\n            \n            loss=lossentropy_fn(pred,y)\n            track_loss+=loss.item()\n            \n            running_loss=track_loss/(i+(x.shape[0]/batch_size))\n    \n    return round(running_loss,4), round(running_acc,2)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T04:13:46.358546Z","iopub.execute_input":"2024-03-28T04:13:46.358825Z","iopub.status.idle":"2024-03-28T04:13:46.378385Z","shell.execute_reply.started":"2024-03-28T04:13:46.358801Z","shell.execute_reply":"2024-03-28T04:13:46.377481Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"n_epochs=20\n\nprint(\"-----------------------Auto Encoder 1------------------------\")\nfor i in range(n_epochs):\n    train_loss=train_e1d1()\n    eval_loss,x,pred=eval_e1d1()\n    print(\"Epoch=\", i+1,\", Train Loss=\",train_loss,\", Eval Loss=\",eval_loss,sep=\"\")\n    \nplt.figure(figsize=(3.2,2.4))\n\nr=torch.randint(low=0,high=pred.shape[0],size=(1,)).item()\n\nplt.subplot(1,2,1)\nplt.title(\"Orignal\")\nplt.imshow(torch.reshape(x[r],shape=(28,28)).cpu())\n\nplt.subplot(1,2,2)\nplt.title(\"Prediction\")\nplt.imshow(torch.reshape(pred[r],shape=(28,28)).cpu())\nplt.show()\n    \nprint(\"-----------------------Auto Encoder 2------------------------\")\nfor i in range(n_epochs):\n    print(\"Epoch=\", i+1,\", Train Loss=\",train_e2d2(),\", Eval Loss=\",eval_e2d2(),sep=\"\") \n\n\nprint(\"-----------------------Classifier Only------------------------\")\nfor i in range(n_epochs):\n    print(\"Epoch=\", i+1,\", Train Loss & Accuracy=\",train_clf(),\", Eval Loss & Accuracy=\",eval_clf(),sep=\"\") \n\n\nprint(\"--------------------Fine-Tuning Whole Metwork------------------\")\nfor i in range(n_epochs):\n    print(\"Epoch=\", i+1,\", Train Loss & Accuracy=\",train_whole(),\", Eval Loss & Accuracy=\",eval_whole(),sep=\"\") \n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T04:13:46.379480Z","iopub.execute_input":"2024-03-28T04:13:46.379778Z","iopub.status.idle":"2024-03-28T04:26:15.348372Z","shell.execute_reply.started":"2024-03-28T04:13:46.379754Z","shell.execute_reply":"2024-03-28T04:26:15.347432Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"-----------------------Auto Encoder 1------------------------\nEpoch=1, Train Loss=0.0223, Eval Loss=0.0088\nEpoch=2, Train Loss=0.0067, Eval Loss=0.0053\nEpoch=3, Train Loss=0.0048, Eval Loss=0.0042\nEpoch=4, Train Loss=0.0039, Eval Loss=0.0037\nEpoch=5, Train Loss=0.0034, Eval Loss=0.0034\nEpoch=6, Train Loss=0.003, Eval Loss=0.003\nEpoch=7, Train Loss=0.0028, Eval Loss=0.0028\nEpoch=8, Train Loss=0.0026, Eval Loss=0.0027\nEpoch=9, Train Loss=0.0024, Eval Loss=0.0025\nEpoch=10, Train Loss=0.0023, Eval Loss=0.0024\nEpoch=11, Train Loss=0.0022, Eval Loss=0.0023\nEpoch=12, Train Loss=0.0021, Eval Loss=0.0021\nEpoch=13, Train Loss=0.002, Eval Loss=0.0021\nEpoch=14, Train Loss=0.0019, Eval Loss=0.002\nEpoch=15, Train Loss=0.0018, Eval Loss=0.002\nEpoch=16, Train Loss=0.0018, Eval Loss=0.002\nEpoch=17, Train Loss=0.0017, Eval Loss=0.0018\nEpoch=18, Train Loss=0.0017, Eval Loss=0.0018\nEpoch=19, Train Loss=0.0016, Eval Loss=0.0017\nEpoch=20, Train Loss=0.0016, Eval Loss=0.0017\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 320x240 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"-----------------------Auto Encoder 2------------------------\nEpoch=1, Train Loss=0.2725, Eval Loss=0.0936\nEpoch=2, Train Loss=0.0703, Eval Loss=0.0555\nEpoch=3, Train Loss=0.0532, Eval Loss=0.0498\nEpoch=4, Train Loss=0.0506, Eval Loss=0.0487\nEpoch=5, Train Loss=0.0498, Eval Loss=0.0473\nEpoch=6, Train Loss=0.0494, Eval Loss=0.0476\nEpoch=7, Train Loss=0.0492, Eval Loss=0.0478\nEpoch=8, Train Loss=0.049, Eval Loss=0.047\nEpoch=9, Train Loss=0.0489, Eval Loss=0.047\nEpoch=10, Train Loss=0.0488, Eval Loss=0.0469\nEpoch=11, Train Loss=0.0487, Eval Loss=0.0472\nEpoch=12, Train Loss=0.0486, Eval Loss=0.0467\nEpoch=13, Train Loss=0.0485, Eval Loss=0.0466\nEpoch=14, Train Loss=0.0485, Eval Loss=0.0463\nEpoch=15, Train Loss=0.0484, Eval Loss=0.0459\nEpoch=16, Train Loss=0.0484, Eval Loss=0.0464\nEpoch=17, Train Loss=0.0483, Eval Loss=0.0469\nEpoch=18, Train Loss=0.0483, Eval Loss=0.0464\nEpoch=19, Train Loss=0.0482, Eval Loss=0.0462\nEpoch=20, Train Loss=0.0482, Eval Loss=0.0463\n-----------------------Classifier Only------------------------\nEpoch=1, Train Loss & Accuracy=(1.0067, 75.08), Eval Loss & Accuracy=(0.5372, 87.81)\nEpoch=2, Train Loss & Accuracy=(0.4802, 88.03), Eval Loss & Accuracy=(0.4058, 89.74)\nEpoch=3, Train Loss & Accuracy=(0.3995, 89.25), Eval Loss & Accuracy=(0.358, 90.63)\nEpoch=4, Train Loss & Accuracy=(0.366, 89.92), Eval Loss & Accuracy=(0.3363, 90.85)\nEpoch=5, Train Loss & Accuracy=(0.3471, 90.3), Eval Loss & Accuracy=(0.3322, 91.1)\nEpoch=6, Train Loss & Accuracy=(0.3358, 90.58), Eval Loss & Accuracy=(0.3249, 90.61)\nEpoch=7, Train Loss & Accuracy=(0.3291, 90.66), Eval Loss & Accuracy=(0.315, 91.1)\nEpoch=8, Train Loss & Accuracy=(0.3234, 90.7), Eval Loss & Accuracy=(0.3101, 91.58)\nEpoch=9, Train Loss & Accuracy=(0.319, 90.91), Eval Loss & Accuracy=(0.3083, 91.27)\nEpoch=10, Train Loss & Accuracy=(0.3167, 90.97), Eval Loss & Accuracy=(0.3046, 91.64)\nEpoch=11, Train Loss & Accuracy=(0.3135, 90.97), Eval Loss & Accuracy=(0.3051, 91.59)\nEpoch=12, Train Loss & Accuracy=(0.3112, 91.05), Eval Loss & Accuracy=(0.3003, 91.62)\nEpoch=13, Train Loss & Accuracy=(0.3105, 90.94), Eval Loss & Accuracy=(0.3048, 91.47)\nEpoch=14, Train Loss & Accuracy=(0.3094, 91.09), Eval Loss & Accuracy=(0.3, 91.69)\nEpoch=15, Train Loss & Accuracy=(0.3079, 91.13), Eval Loss & Accuracy=(0.3037, 91.45)\nEpoch=16, Train Loss & Accuracy=(0.307, 91.08), Eval Loss & Accuracy=(0.298, 91.79)\nEpoch=17, Train Loss & Accuracy=(0.3055, 91.18), Eval Loss & Accuracy=(0.3045, 91.06)\nEpoch=18, Train Loss & Accuracy=(0.3058, 91.18), Eval Loss & Accuracy=(0.3, 91.75)\nEpoch=19, Train Loss & Accuracy=(0.3051, 91.23), Eval Loss & Accuracy=(0.3037, 91.52)\nEpoch=20, Train Loss & Accuracy=(0.3037, 91.23), Eval Loss & Accuracy=(0.3013, 91.71)\n--------------------Fine-Tuning Whole Metwork------------------\nEpoch=1, Train Loss & Accuracy=(0.2551, 92.75), Eval Loss & Accuracy=(0.1359, 95.57)\nEpoch=2, Train Loss & Accuracy=(0.0991, 96.92), Eval Loss & Accuracy=(0.0955, 97.16)\nEpoch=3, Train Loss & Accuracy=(0.0683, 97.84), Eval Loss & Accuracy=(0.1009, 97.02)\nEpoch=4, Train Loss & Accuracy=(0.0541, 98.23), Eval Loss & Accuracy=(0.084, 97.57)\nEpoch=5, Train Loss & Accuracy=(0.0436, 98.56), Eval Loss & Accuracy=(0.1004, 97.19)\nEpoch=6, Train Loss & Accuracy=(0.0387, 98.74), Eval Loss & Accuracy=(0.1004, 97.38)\nEpoch=7, Train Loss & Accuracy=(0.0352, 98.85), Eval Loss & Accuracy=(0.094, 97.69)\nEpoch=8, Train Loss & Accuracy=(0.0329, 98.9), Eval Loss & Accuracy=(0.1042, 97.59)\nEpoch=9, Train Loss & Accuracy=(0.0232, 99.26), Eval Loss & Accuracy=(0.1342, 97.2)\nEpoch=10, Train Loss & Accuracy=(0.0268, 99.16), Eval Loss & Accuracy=(0.1067, 97.76)\nEpoch=11, Train Loss & Accuracy=(0.0224, 99.26), Eval Loss & Accuracy=(0.1089, 97.96)\nEpoch=12, Train Loss & Accuracy=(0.0266, 99.2), Eval Loss & Accuracy=(0.1133, 97.79)\nEpoch=13, Train Loss & Accuracy=(0.0187, 99.41), Eval Loss & Accuracy=(0.1302, 97.6)\nEpoch=14, Train Loss & Accuracy=(0.0162, 99.52), Eval Loss & Accuracy=(0.1287, 97.87)\nEpoch=15, Train Loss & Accuracy=(0.022, 99.29), Eval Loss & Accuracy=(0.1355, 97.76)\nEpoch=16, Train Loss & Accuracy=(0.0167, 99.47), Eval Loss & Accuracy=(0.1176, 98.03)\nEpoch=17, Train Loss & Accuracy=(0.0196, 99.42), Eval Loss & Accuracy=(0.1212, 97.87)\nEpoch=18, Train Loss & Accuracy=(0.0152, 99.54), Eval Loss & Accuracy=(0.1464, 97.62)\nEpoch=19, Train Loss & Accuracy=(0.0141, 99.57), Eval Loss & Accuracy=(0.1422, 97.85)\nEpoch=20, Train Loss & Accuracy=(0.0197, 99.43), Eval Loss & Accuracy=(0.1303, 97.7)\n","output_type":"stream"}]}]}